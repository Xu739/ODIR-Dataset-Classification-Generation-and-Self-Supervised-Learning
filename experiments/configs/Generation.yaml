# Model Configuration
model: 'ddim'  # Model type, choices: ['cGAN', 'ddim',]
CUDA_VISIBLE_DEVICES: '4'  # GPU device ID to use

# Dataset Paths
datafile_path: './data'   # Root directory for data files
img_path: './data/ODIR/ODIR-5K/Training Images'  # Path to training images
dataset: 'ODIR'           # Options: ['cifar-10', 'ODIR','mnist']

# Data Splitting (for cross-validation)
split_num: 5  # Total number of splits/folds
split_id: 1  # Current split ID to use (1-based index)

# Image Processing
img_size: 128  # Input image size (height/width in pixels)
batch_size: 8  # Number of samples per batch
num_workers: 32  # Number of subprocesses for data loading
is_oversample: 1  # Whether to use oversampling (0=False, 1=True)

# cGAN-specific Parameters
latent_dim: 100  # Dimension of latent space/noise vector
num_classes: 8  # Number of output classes
criterion: 'BCELoss'#Criterion type, choices: ['BCELoss']

# DDIM-specific Parameters (Diffusion Models)
T: 1000  # Number of diffusion timesteps
beta_start: 0.0001  # Starting noise level
beta_end: 0.02  # Ending noise level
debug_shapes: False #debug UNet shape
drop_prob : 0.1


# Optimizer Parameters
lr: 0.0001  # Base learning rate
optim: "adam"  # Optimizer type, choices: ['adam', 'SGD', 'rsm']
momentum: 0.9  # Momentum factor (for SGD)
weight_decay: 0.00001  # L2 regularization weight

# Training Control
is_resume: 0  # Whether to resume training (0=False, 1=True)
start_epoch: 138  # Epoch number to start/resume from
epochs: 200  # Total number of training epochs

# Learning Rate Scheduling
scheduler: 'MultistepLR'  # Learning rate scheduler type
milestones: [50,]         # Epochs at which to adjust LR
gamma: 0.5                # Factor by which to multiply LR at milestones

# Generator/Discriminator Learning Rates
d_lr: 0.0001  # Discriminator learning rate (for GANs)
g_lr: 0.0001  # Generator learning rate (for GANs)